{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis, levy_stable\n",
    "import statsmodels.api as sm\n",
    "from arch import arch_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from hurst import compute_Hc\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "from arch.univariate.base import ConvergenceWarning\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import numpy2ri\n",
    "numpy2ri.activate()\n",
    "fBasics = importr('fBasics')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Data/'\n",
    "files = {\n",
    "    'bb_bond': 'BB Bond.csv',\n",
    "    'bb_commodity': 'BB Commodity.csv',\n",
    "    'bb_crypto': 'BB Crypto.csv',\n",
    "    'bb_exchange_rate': 'BB Exchange Rate.csv',\n",
    "    'bb_real_estate': 'BB Real Estate.csv',\n",
    "    'bb_stock': 'BB Stock.csv'\n",
    "}\n",
    "\n",
    "for var_name, file_name in files.items():\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "    empty_cols = df.columns[df.isnull().all()].tolist()\n",
    "    df = df.drop(columns=empty_cols)\n",
    "    globals()[var_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = pd.concat([bb_stock, bb_exchange_rate, bb_commodity, bb_bond, bb_real_estate], axis=1)\n",
    "cc = pd.read_csv('Data/CC Prices.csv', index_col=0, parse_dates=True)\n",
    "cc_cap = pd.read_csv('Data/CC Market Cap.csv', index_col=0, parse_dates=True)\n",
    "cc_list = pd.read_csv('Data/CC List.csv')\n",
    "\n",
    "dd = pd.concat([cc, bb], axis=1, join='inner')\n",
    "dd[dd <= 0] = np.nan\n",
    "dd = dd[dd.columns[dd.notna().sum() > 2000]]\n",
    "dd.to_csv('Data/DD price.csv', index=True)\n",
    "cc = cc.reindex(dd.index)\n",
    "\n",
    "dd_return = np.log(dd / dd.shift(1))\n",
    "dd_return = dd_return.iloc[1:]\n",
    "dd_return = dd_return.drop(columns=dd_return.loc[:, (dd_return == 0).sum() > 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_source = {}\n",
    "\n",
    "for category, df in {\n",
    "    'Stock': bb_stock,\n",
    "    'Exchange Rate': bb_exchange_rate,\n",
    "    'Commodity': bb_commodity,\n",
    "    'Bond': bb_bond,\n",
    "    'Real Estate': bb_real_estate,\n",
    "    'Crypto': cc\n",
    "}.items():\n",
    "    for col in df.columns:\n",
    "        column_source[col] = category\n",
    "\n",
    "dd_index = pd.DataFrame(list(column_source.items()), columns=['Asset', 'Type'])\n",
    "dd_index = dd_index[dd_index['Asset'].isin(dd.columns)]\n",
    "\n",
    "dd_index.to_csv('Data/DD Index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_window = int(len(dd_return) / 3)\n",
    "\n",
    "start_time = time.time()\n",
    "print(datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "window_size = 250\n",
    "step_size = 21\n",
    "results = []\n",
    "\n",
    "pause_step = 0\n",
    "\n",
    "for window_size in tqdm(range(start_window, len(dd_return) + 1, step_size), desc=\"Extending windows\", ncols=100):\n",
    "    start = 0\n",
    "    dd_window = dd_return.iloc[start:start + window_size]\n",
    "    dd_date = dd_window.index[-1].strftime('%Y-%m-%d')\n",
    "    dd_window = dd_window.loc[:, dd_window.notnull().all()]\n",
    "    dd_window = dd_window.drop(columns=dd_window.loc[:, (dd_window == 0).sum() > (window_size / 10)])\n",
    "    dd_window = dd_window.loc[:, dd_window.std() != 0]\n",
    "    if dd_window.shape[1] == 0:\n",
    "        continue\n",
    "\n",
    "    dd_factors = pd.DataFrame({\"Date\": [dd_window.index[-1]] * len(dd_window.columns), \"Asset\": dd_window.columns})\n",
    "    dd_factors = dd_factors.merge(dd_index, on=\"Asset\", how=\"left\")\n",
    "\n",
    "    quantile_levels = [0.005, 0.01, 0.025, 0.05, 0.95, 0.975, 0.99, 0.995]\n",
    "    for q in quantile_levels:\n",
    "        dd_factors[\"Q_\" + str(q).replace(\".\", \"_\")] = dd_window.quantile(q).values\n",
    "    for q in quantile_levels[:4]:\n",
    "        dd_factors[\"CTE_\" + str(q).replace(\".\", \"_\")] = dd_window[dd_window <= dd_window.quantile(q)].mean().values\n",
    "    for q in quantile_levels[4:]:\n",
    "        dd_factors[\"CTE_\" + str(q).replace(\".\", \"_\")] = dd_window[dd_window >= dd_window.quantile(q)].mean().values\n",
    "\n",
    "    figarch_list = []\n",
    "    for col in dd_window.columns:\n",
    "        model = arch_model(dd_window[col], vol=\"FIGARCH\", p=1, q=1, rescale=False)\n",
    "        result = model.fit(disp=\"off\")\n",
    "        d_param = result.params.get(\"d\", np.nan)\n",
    "        figarch_list.append(d_param)\n",
    "\n",
    "    dd_factors[\"FIGARCH_d\"] = figarch_list\n",
    "\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "    stable_res = pd.DataFrame(\n",
    "        Parallel(n_jobs=num_cores)(\n",
    "            delayed(lambda d: (lambda r: (r[0], r[3]))(levy_stable.fit(d)))(dd_window[col].dropna().values)\n",
    "            for col in dd_window.columns\n",
    "        ),\n",
    "        index=dd_window.columns, columns=[\"Stable_alpha\", \"Stable_gamma\"]\n",
    "    )\n",
    "\n",
    "    dd_factors = dd_factors.merge(stable_res, left_on=\"Asset\", right_index=True, how=\"left\")\n",
    "    dd_factors[\"Variance\"] = dd_window.var().values\n",
    "    dd_factors[\"Skewness\"] = dd_window.apply(skew).values\n",
    "    dd_factors[\"Kurtosis\"] = dd_window.apply(kurtosis).values\n",
    "    dd_factors[\"ACF_Lag1\"] = dd_window.apply(lambda x: sm.tsa.acf(x.dropna(), nlags=1, fft=False)[1] if len(x.dropna()) > 1 else 0).values\n",
    "    dd_factors[\"Hurst\"] = dd_window.apply(lambda x: compute_Hc(x, kind=\"change\", simplified=False)[0] if len(x.dropna()) >= 20 else np.nan).values\n",
    "\n",
    "    output_path = os.path.join(f\"Factors/{dd_date} Factors.csv\")\n",
    "    dd_factors.to_csv(output_path, index=False)\n",
    "    results.append(dd_factors)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(datetime.fromtimestamp(start).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(f\"Execution completed in {total_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 11:30:38\n"
     ]
    }
   ],
   "source": [
    "start_window = int(len(dd_return) / 3)\n",
    "\n",
    "start_time = time.time()\n",
    "print(datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "window_size = 250\n",
    "step_size = 21\n",
    "results = []\n",
    "\n",
    "pause_step = 0\n",
    "\n",
    "# window_size = 2587\n",
    "window_size = int(len(dd_return) / 3)\n",
    "\n",
    "start = 0\n",
    "dd_window = dd_return.iloc[start:start + window_size]\n",
    "dd_date = dd_window.index[-1].strftime('%Y-%m-%d')\n",
    "dd_window = dd_window.loc[:, dd_window.notnull().all()]\n",
    "dd_window = dd_window.drop(columns=dd_window.loc[:, (dd_window == 0).sum() > (window_size / 10)])\n",
    "dd_window = dd_window.loc[:, dd_window.std() != 0]\n",
    "\n",
    "dd_factors = pd.DataFrame({\"Date\": [dd_window.index[-1]] * len(dd_window.columns), \"Asset\": dd_window.columns})\n",
    "dd_factors = dd_factors.merge(dd_index, on=\"Asset\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "stable_res = pd.DataFrame(\n",
    "    Parallel(n_jobs=num_cores)(\n",
    "        delayed(lambda d: (lambda r: (r[0], r[3]))(levy_stable.fit(d)))(dd_window[col].dropna().values)\n",
    "        for col in dd_window.columns\n",
    "    ),\n",
    "    index=dd_window.columns, columns=[\"Stable_alpha\", \"Stable_gamma\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dd_window['bitcoin-plus'].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
